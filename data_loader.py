import pandas as pd
import numpy as np
import requests
import os
import json
from collections import defaultdict
from functools import reduce
import io
import zipfile

# data_loader.py
# This script is responsible for fetching, processing, and enriching financial data from the SEC EDGAR database.
# It contains the core logic for parsing XBRL data, applying calculation formulas to fill missing values,
# and creating standardized financial statements.

EMAIL = "your_email@example.com"

# Load the CIK dictionary, which maps company tickers to their Central Index Key.
CIK_dict = pd.read_csv("CIK_dict.csv", converters={"cik_str": str})

# Load the comprehensive set of calculation formulas generated by xbrl_main.py.
# This file contains the parent-child relationships for financial metrics as defined in the US-GAAP XBRL taxonomy.
with open("xbrl_prep/income_statement_formulas.json", "r") as f:
    calculation_formulas = json.load(f)

# The inverted index is needed again to find all possible calculation paths for the UI.
calculation_formulas_inverted = defaultdict(list)
for parent, formulas in calculation_formulas.items():
    for formula in formulas:
        for child_info in formula["children"]:
            child_name = child_info["child"]
            calculation_formulas_inverted[child_name].append(parent)

# --- NEW: Centralized mapping for our Standardized Income Statement ---
# This dictionary is now the single source of truth for all metrics needed to build the income statement.
# It's used by both the solver to find the data and the statement creator to build the table.
INCOME_STATEMENT_MAPPING = {
    # Revenue and its primary calculation components
    "Revenue": ["SalesRevenueNet", "Revenues", "RevenueFromContractWithCustomer"],
    "SalesRevenueNet": ["SalesRevenueNet"],
    "Revenues": ["Revenues"],
    "RevenueFromContractWithCustomer": ["RevenueFromContractWithCustomer"],
    "RevenueFromContractWithCustomerIncludingAssessedTax": ["RevenueFromContractWithCustomerIncludingAssessedTax"],
    "RevenueFromContractWithCustomerExcludingAssessedTax": ["RevenueFromContractWithCustomerExcludingAssessedTax"],
    "RevenueNotFromContractWithCustomer": ["RevenueNotFromContractWithCustomer"],
    "InterestIncomeExpenseAfterProvisionForLoanLoss": ["InterestIncomeExpenseAfterProvisionForLoanLoss"],
    "NoninterestIncome": ["NoninterestIncome"],
    # Cost of Revenue and its granular components
    "Cost of Revenue": ["CostOfRevenue", "CostOfGoodsAndServicesSold"],
    "CostOfProductAndServiceSold": ["CostOfProductAndServiceSold"],
    "FinancingInterestExpense": ["FinancingInterestExpense"],
    "ProvisionForLoanLeaseAndOtherLosses": ["ProvisionForLoanLeaseAndOtherLosses"],
    "PolicyholderBenefitsAndClaimsIncurredNet": ["PolicyholderBenefitsAndClaimsIncurredNet"],
    "LiabilityForFuturePolicyBenefitsPeriodExpenseIncome": ["LiabilityForFuturePolicyBenefitsPeriodExpenseIncome"],
    "PolicyholderAccountBalanceInterestExpense": ["PolicyholderAccountBalanceInterestExpense"],
    "PolicyholderDividendsExpense": ["PolicyholderDividendsExpense"],
    "DeferredSalesInducementCostAmortizationExpense": ["DeferredSalesInducementCostAmortizationExpense"],
    "PresentValueOfFutureInsuranceProfitsAmortizationExpense": ["PresentValueOfFutureInsuranceProfitsAmortizationExpense"],
    "AmortizationOfMortgageServicingRightsMSRs": ["AmortizationOfMortgageServicingRightsMSRs"],
    "DeferredPolicyAcquisitionCostsAmortizationExpense": ["DeferredPolicyAcquisitionCostsAmortizationExpense"],
    "AmortizationOfValueOfBusinessAcquiredVOBA": ["AmortizationOfValueOfBusinessAcquiredVOBA"],
    "OtherCostOfOperatingRevenue": ["OtherCostOfOperatingRevenue"],
    "MerchantMarineOperatingDifferentialSubsidy": ["MerchantMarineOperatingDifferentialSubsidy"],
    # Core statement items
    "Gross Profit": ["GrossProfit"],
    "R&D Expenses": ["ResearchAndDevelopmentExpense"],
    "SG&A Expenses": ["SellingGeneralAndAdministrativeExpense"],
    "Operating Expenses": ["OperatingExpenses"],
    "Operating Income": ["OperatingIncomeLoss"],
    # Non-Operating Section
    "Interest Income": ["InterestIncome", "InterestAndDividendIncome", "InterestIncomeOperating"],
    "Interest Expense": ["InterestExpense", "InterestExpenseOperating"],
    "Income from Equity Method Investments": ["IncomeLossFromEquityMethodInvestments"],
    "Other Non-operating Income (Expense)": ["NonoperatingIncomeExpense", "OtherNonoperatingIncomeExpense"],
    # Pre-Tax and Bottom Line
    "Income Before Taxes": ["IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest", "IncomeLossFromContinuingOperationsBeforeIncomeTax"],
    "Taxes": ["IncomeTaxExpenseBenefit"],
    "Net Income": ["NetIncomeLoss", "ProfitLoss", "NetIncomeLossAvailableToCommonStockholdersBasic"],
    # NEW: Add Earnings Per Share metrics
    "Basic EPS": ["EarningsPerShareBasic"],
    "Diluted EPS": ["EarningsPerShareDiluted"],
}


def get_company_info(ticker):
    """
    Retrieves company info (title, CIK) for a given ticker symbol.
    """
    row = CIK_dict[CIK_dict["ticker"].str.upper() == ticker.upper()]
    if not row.empty:
        return {
            "title": row["title"].iloc[0],
            "cik_str": row["cik_str"].iloc[0],
        }
    return None


def get_cik(ticker):
    """
    Retrieves the Central Index Key (CIK) for a given company ticker symbol.
    """
    info = get_company_info(ticker)
    if info:
        return info["cik_str"]
    else:
        raise ValueError(f"Ticker {ticker} not found in CIK_dict")


def get_filing_by_metrics(CIK):
    """
    Fetches a company's entire fact history from the SEC EDGAR API.

    Args:
        CIK (str): The company's Central Index Key.

    Returns:
        dict: A dictionary containing all the us-gaap facts for the company.
    """
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{CIK}.json"
    headers = {"User-Agent": EMAIL}
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raise an exception for HTTP errors
    data = response.json()
    return data["facts"]["us-gaap"]


def extract_metrics(filing_metrics: dict):
    """
    Extracts and filters all annual data points from the raw SEC filing.
    This function is now more strict, ensuring that only full-year data from
    10-K filings without quarterly frames is included.
    """
    metric_master = {}
    for metric, attributes in filing_metrics.items():
        for unit, entries in attributes["units"].items():
            for entry in entries:
                # Strict filtering for annual 10-K data.
                # 1. Must be a 10-K form.
                # 2. Fiscal Period ('fp') must be 'FY'. This is the primary indicator.
                # 3. As a safeguard, the 'frame' attribute, if it exists, must not indicate a quarter.
                is_annual_10k = (
                    entry.get("form") == "10-K" and
                    entry.get("fp") == "FY"
                )

                if is_annual_10k:
                    # Additional safeguard to explicitly ignore any data points framed as a quarter.
                    frame = entry.get('frame', '')
                    if 'Q' in frame:
                        continue

                    metric_key = f"{metric}_{unit}"
                    if metric_key not in metric_master:
                        metric_master[metric_key] = []
                    metric_master[metric_key].append(entry)
    return metric_master


def format_metrics_efficient(extracted_metrics, target_metrics=None):
    """
    Converts extracted metrics into a clean, graphable DataFrame.
    This version processes data based on 'end' dates to accurately represent
    financial data, handles duplicates by prioritizing the most recent filing,
    and then applies a deep recursive solver to intelligently fill missing values.
    """
    all_entries = []
    # Iterate through each metric and its corresponding entries from the extracted data.
    for metric_name, entries in extracted_metrics.items():
        for entry in entries:
            # Create a dictionary for each entry, standardizing the keys.
            record = {
                "metric": metric_name,
                "end": entry["end"],
                "val": entry["val"],
                "filed": entry["filed"],
            }
            all_entries.append(record)

    if not all_entries:
        return pd.DataFrame(), {}

    # Create a DataFrame from the list of all entries.
    df = pd.DataFrame(all_entries)

    # Convert date-like columns to datetime objects for proper sorting and manipulation.
    df["filed"] = pd.to_datetime(df["filed"])
    df["end"] = pd.to_datetime(df["end"])

    # Sort entries by the filing date in descending order. This ensures that for any duplicates,
    # the one from the most recent filing appears first.
    df.sort_values(by="filed", ascending=False, inplace=True)

    # Remove duplicate entries for the same metric and end date.
    # By keeping the 'first' entry, we ensure that we use the data from the most recent filing.
    df.drop_duplicates(subset=["metric", "end"], keep="first", inplace=True)

    # Pivot the DataFrame to create a time-series format with 'end' date as the index.
    df_pivot = df.pivot_table(index="end", columns="metric", values="val")

    # Ensure the DataFrame is sorted by date, which is crucial for identifying the latest year.
    df_pivot.sort_index(inplace=True)

    # Remove rows with too many N/A values, but *always* keep the latest year's data,
    # as it is the most recent even if sparse.
    if not df_pivot.empty:
        latest_year_row = df_pivot.iloc[-1:]
        # Apply the sparsity filter to all other rows.
        df_pivot_other_rows = df_pivot.iloc[:-1]
        
        # This will produce a SettingWithCopyWarning, which is expected here and can be ignored.
        # The operation is safe in this context.
        df_pivot_other_rows.dropna(thresh=df_pivot_other_rows.shape[1] * 0.3, inplace=True)
        
        # Re-combine the filtered data with the latest year's data.
        df_pivot = pd.concat([df_pivot_other_rows, latest_year_row])


    # Apply the deep recursive solver to intelligently fill in missing values.
    # This is the core of the data enrichment process.
    df_solved, all_results = apply_formulas_deep_dfs(
        df_pivot,
        calculation_formulas,
        target_metrics=target_metrics
    )

    # If the solving process creates duplicate columns, consolidate them.
    df_solved = df_solved.groupby(by=df_solved.columns, axis=1).first()

    # Reset index to make 'end' a regular column for easier plotting.
    df_solved.reset_index(inplace=True)
    df_solved.rename_axis(None, axis=1, inplace=True)

    # Format the 'end' column to show only the date part.
    df_solved["end"] = pd.to_datetime(df_solved["end"]).dt.date

    # Sort the final dataframe by the end date.
    df_solved.sort_values(by="end", ascending=True, inplace=True)

    return df_solved, all_results


def find_col_with_units(base_metric, df_columns):
    """
    Finds the full column name in a DataFrame that corresponds to a base metric name.
    It prioritizes '_USD' suffixes as they are the most common for financial statements.

    Args:
        base_metric (str): The base name of the metric (e.g., "Revenues").
        df_columns (list): The list of all column names in the DataFrame.

    Returns:
        str or None: The full column name if found, otherwise None.
    """
    # Prioritize metrics with _USD unit as they are most common for income statements
    if base_metric + "_USD" in df_columns:
        return base_metric + "_USD"
    for col in df_columns:
        if col.startswith(base_metric + "_"):
            return col
    if base_metric in df_columns:
        return base_metric
    return None


def apply_formulas_deep_dfs(df, all_formulas, target_metrics=None):
    """
    Applies XBRL calculation formulas using a recursive Depth-First Search (DFS) algorithm.
    It now calculates all viable formulas for each metric and stores them for the UI.
    """
    df_enriched = df.copy()
    cache = {}
    # This new cache will store all successful calculation results for each metric/year.
    all_results_cache = defaultdict(list)

    def get_sorted_formulas(formulas, year, visited):
        """
        Scores a list of candidate formulas and returns them sorted from best to worst.
        The score is a tuple that prioritizes formulas with more children already present
        in the raw data, then by the number of solvable children, and finally by shallowness.
        """
        scored_formulas = []
        for formula_obj in formulas:
            child_list = formula_obj["formula"]
            total_children = len(child_list)
            if total_children == 0:
                continue

            present_in_df = 0
            solvable = 0

            # If any required component is already in the current recursion stack, this
            # formula would cause a cycle, so we disqualify it immediately.
            is_cyclic = any((child["child"], year) in visited for child in child_list)
            if is_cyclic:
                continue

            for child in child_list:
                child_name = child["child"]
                col_name = find_col_with_units(child_name, df_enriched.columns)

                # Highest priority: is the data already in the dataframe?
                if col_name and pd.notnull(df_enriched.at[year, col_name]):
                    present_in_df += 1
                # Second priority: is it solvable via another formula?
                elif child_name in all_formulas:
                    solvable += 1

            # The score is a tuple that prioritizes existing data above all.
            # Python's max() on a list of tuples will sort by each element in order.
            score = (present_in_df, solvable, -total_children)
            scored_formulas.append({"formula_obj": formula_obj, "score": score})

        # Sort the formulas from best to worst score.
        return sorted(scored_formulas, key=lambda x: x["score"], reverse=True)

    def solve_for_cell(metric_base, year, visited):
        """
        Recursively attempts to solve for a single metric in a single year (a "cell").
        It calculates all viable formulas, stores them, and returns the highest-scored result.
        """
        # --- Base Cases and Memoization ---
        if (metric_base, year) in visited:
            return None  # Cycle detected
        visited.add((metric_base, year))

        if (metric_base, year) in cache:
            return cache.get((metric_base, year))

        col_name = find_col_with_units(metric_base, df_enriched.columns)
        if col_name and pd.notnull(df_enriched.at[year, col_name]):
            val = df_enriched.at[year, col_name]
            cache[(metric_base, year)] = val
            return val

        # --- Forward Calculation (Solving a parent from its children) ---
        primary_result = None
        if metric_base in all_formulas:
            candidate_formulas = [
                {"type": "forward", "formula": f["children"], "parent": metric_base}
                for f in all_formulas[metric_base]
            ]
            sorted_formulas = get_sorted_formulas(candidate_formulas, year, visited)

            # Iterate through sorted formulas, calculate all viable results, and store them.
            for scored_formula in sorted_formulas:
                formula_obj = scored_formula["formula_obj"]
                child_values = []
                all_children_solved = True

                formula_parts = []
                for child in formula_obj["formula"]:
                    val = solve_for_cell(child["child"], year, visited.copy())
                    if val is None:
                        all_children_solved = False
                        break
                    child_values.append({"value": val, "weight": child["weight"]})

                    # Build formula string for display
                    sign = "+" if child["weight"] > 0 else "-"
                    child_name_part = child["child"]
                    weight_part = (
                        f"{abs(child['weight'])} * "
                        if abs(child['weight']) != 1
                        else ""
                    )
                    formula_parts.append(f"{sign} {weight_part}{child_name_part}")

                if all_children_solved:
                    result = sum(cv["value"] * cv["weight"] for cv in child_values)
                    formula_str = " ".join(formula_parts).lstrip("+ ")

                    # Store this successful result in the new cache.
                    all_results_cache[(metric_base, year)].append(
                        {"source": f"Calc: {formula_str}", "value": result}
                    )

                    # The first successful result (from the best-scored formula) is the primary one.
                    if primary_result is None:
                        primary_result = result

        # Cache and return only the primary result for other solver calculations.
        cache[(metric_base, year)] = primary_result

        if primary_result is not None:
            return primary_result

        # If we get here, no formula succeeded to produce a primary_result.
        # We should not assume 0 for leaf nodes, as a missing leaf is unknown, not zero.
        # We will let the final value be None (it will be NaN in the DataFrame),
        # which correctly represents missing data.
        cache[(metric_base, year)] = None
        return None

    # --- Main Execution Loop ---
    # THE CRITICAL FIX: The solver must consider ALL metrics available in the data AND
    # the formulas as potential starting points. This ensures that even if a metric
    # in the data isn't a parent in the formulas, its value is still available
    # for calculations where it acts as a child.
    all_data_metrics = {
        col.replace("_USD", "").split("_")[0] for col in df_enriched.columns
    }
    all_formula_metrics = set(list(all_formulas.keys()))
    all_metrics_to_solve = all_data_metrics.union(all_formula_metrics)

    # NEW: Add the metrics we care about for our standardized statements. This forces
    # the solver to proactively look for every component we might need.
    if target_metrics:
        all_metrics_to_solve.update(target_metrics)

    for metric in all_metrics_to_solve:
        col_name = find_col_with_units(metric, df_enriched.columns)
        # Optimization: If the column already exists and is fully populated, skip it.
        if col_name and df_enriched[col_name].notna().all():
            continue

        for year in df_enriched.index:
            # Only try to solve if the value is actually missing.
            if not (col_name and pd.notnull(df_enriched.at[year, col_name])):
                # The `top_level_target` is no longer needed.
                solved_value = solve_for_cell(metric, year, set())
                if solved_value is not None:
                    # IMPORTANT: Update the DataFrame immediately after solving a cell.
                    # This makes the newly calculated value available for subsequent calculations
                    # in the same run, which is critical for complex dependencies.
                    final_col_name = find_col_with_units(
                        metric, df_enriched.columns
                    ) or (metric + "_USD")
                    if final_col_name not in df_enriched:
                        df_enriched[final_col_name] = np.nan
                    df_enriched.at[year, final_col_name] = solved_value

    return df_enriched, all_results_cache


def process_metrics(ticker: str):
    """
    A convenience wrapper function that chains together all the steps required
    to fetch and process financial data for a given ticker.
    This now returns the comprehensive solved dataframe, the standard income statement,
    and a dictionary of alternative calculations.
    """
    df_raw_extracted = extract_metrics(get_filing_by_metrics(get_cik(ticker)))

    # UPDATED: We now pass the full list of required metrics to the processing function.
    # This ensures the solver proactively looks for every piece of data we might need.
    all_target_metrics = set()
    for possibilities in INCOME_STATEMENT_MAPPING.values():
        for metric in possibilities:
            all_target_metrics.add(metric)

    # format_metrics_efficient now returns the solved df with all metrics
    df_solved, all_results = format_metrics_efficient(df_raw_extracted, list(all_target_metrics))

    # create_standard_income_statement takes the solved df and the cache
    standard_is_df, alternatives, standard_metrics_list = (
        create_standard_income_statement(df_solved, all_results)
    )
    
    # NEW: Calculate key ratios from the standardized income statement
    key_ratios_df = calculate_key_ratios(standard_is_df)

    # NEW: Merge the standardized, clean-named metrics back into the main DataFrame.
    # The standardized_is_df has 'end' dates as its index.
    if not standard_is_df.empty:
        # We need to set 'end' as the index on the main df to merge correctly.
        df_solved.set_index('end', inplace=True)
        # The standardized df columns will overwrite any raw data columns with the same name.
        df_merged = standard_is_df.join(df_solved, how='outer', lsuffix='_std')
        df_merged.reset_index(inplace=True)
        # FIX: The reset_index() call creates a column named 'index'. We must rename it back to 'end'.
        df_merged.rename(columns={'index': 'end'}, inplace=True)
    else:
        df_merged = df_solved

    return df_merged, standard_is_df, alternatives, standard_metrics_list, key_ratios_df


def create_standard_income_statement(df, all_results):
    """
    Creates a standardized income statement from the fully solved financial data.
    This function uses an iterative, bi-directional calculation engine to solve for
    missing values, ensuring the most complete and accurate statement possible.
    """
    if df is None or df.empty:
        return pd.DataFrame(), {}, []

    df_indexed = df.copy()
    if "end" in df_indexed.columns:
        # Sort by date descending to have the latest year on the left.
        df_indexed = df_indexed.set_index("end").sort_index(ascending=False)

    # The mapping is now defined globally, so we can just use it here.
    mapping = INCOME_STATEMENT_MAPPING

    # Helper to find the first available series for a list of possible metric names
    def find_series(std_name):
        for p in mapping.get(std_name, []):
            col = find_col_with_units(p, df_indexed.columns)
            if col:
                return df_indexed[col]
        return pd.Series(np.nan, index=df_indexed.index, name=std_name)

    # --- Step 1: Build initial DataFrame with all available data ---
    standard_df = pd.DataFrame(index=df_indexed.index)
    for std_name in mapping:
        standard_df[std_name] = find_series(std_name)

    # --- Step 2: Iterative, Bi-Directional Calculation Engine ---
    for _ in range(5):  # Loop multiple times to fill cascading gaps
        # --- Revenue Calculation ---
        primary_revenue = standard_df["SalesRevenueNet"].combine_first(
            standard_df["Revenues"]
        ).combine_first(standard_df["RevenueFromContractWithCustomer"])

        comp1_1 = standard_df["RevenueFromContractWithCustomerIncludingAssessedTax"]
        comp1_2 = standard_df["RevenueNotFromContractWithCustomer"]
        valid_rows_1 = comp1_1.notna() | comp1_2.notna()
        calc_rev_1 = (comp1_1.fillna(0) + comp1_2.fillna(0)).where(valid_rows_1)

        comp2_1 = standard_df["RevenueFromContractWithCustomerExcludingAssessedTax"]
        comp2_2 = standard_df["RevenueNotFromContractWithCustomer"]
        valid_rows_2 = comp2_1.notna() | comp2_2.notna()
        calc_rev_2 = (comp2_1.fillna(0) + comp2_2.fillna(0)).where(valid_rows_2)
        
        standard_df["Revenue"] = primary_revenue.combine_first(calc_rev_1).combine_first(calc_rev_2)

        # --- Cost of Revenue Calculation ---
        primary_cor = standard_df["Cost of Revenue"]
        
        granular_cor_components = [
            "CostOfProductAndServiceSold", "FinancingInterestExpense", "ProvisionForLoanLeaseAndOtherLosses",
            "PolicyholderBenefitsAndClaimsIncurredNet", "LiabilityForFuturePolicyBenefitsPeriodExpenseIncome",
            "PolicyholderAccountBalanceInterestExpense", "PolicyholderDividendsExpense", "DeferredSalesInducementCostAmortizationExpense",
            "PresentValueOfFutureInsuranceProfitsAmortizationExpense", "AmortizationOfMortgageServicingRightsMSRs",
            "DeferredPolicyAcquisitionCostsAmortizationExpense", "AmortizationOfValueOfBusinessAcquiredVOBA",
            "OtherCostOfOperatingRevenue", "MerchantMarineOperatingDifferentialSubsidy"
        ]
        component_series_list = [standard_df[name] for name in granular_cor_components if name in standard_df]

        calc_cor = pd.Series(np.nan, index=standard_df.index)
        if component_series_list:
            components_df = pd.concat(component_series_list, axis=1)
            valid_rows = components_df.notna().any(axis=1)
            if valid_rows.any():
                calc_cor = components_df.fillna(0).sum(axis=1).where(valid_rows)
        
        standard_df["Cost of Revenue"] = primary_cor.combine_first(calc_cor).fillna(standard_df["Revenue"] - standard_df["Gross Profit"])

        # --- Gross Profit Calculation ---
        standard_df["Gross Profit"] = standard_df["Gross Profit"].fillna(standard_df["Revenue"] - standard_df["Cost of Revenue"])

        # --- Operating Expenses Identities ---
        standard_df["Operating Expenses"] = standard_df["Operating Expenses"].fillna(standard_df["R&D Expenses"].fillna(0) + standard_df["SG&A Expenses"].fillna(0))
        standard_df["Operating Income"] = standard_df["Operating Income"].fillna(standard_df["Gross Profit"] - standard_df["Operating Expenses"])
        standard_df["Operating Expenses"] = standard_df["Operating Expenses"].fillna(standard_df["Gross Profit"] - standard_df["Operating Income"])

        # --- Non-Operating & Pre-Tax Identities ---
        known_non_op_items = (
            standard_df["Interest Income"].fillna(0)
            - standard_df["Interest Expense"].fillna(0)
            + standard_df["Income from Equity Method Investments"].fillna(0)
            + standard_df["Other Non-operating Income (Expense)"].fillna(0)
        )
        standard_df["Income Before Taxes"] = standard_df["Income Before Taxes"].fillna(standard_df["Operating Income"] + known_non_op_items)
        standard_df["Net Income"] = standard_df["Net Income"].fillna(standard_df["Income Before Taxes"] - standard_df["Taxes"].fillna(0))

    # --- Step 3: Final Plug Calculations ---
    # Calculate 'Other Operating Expenses' as the final reconciling item.
    if "Operating Expenses" in standard_df:
        other_op_ex = (
            standard_df["Operating Expenses"]
            - standard_df["R&D Expenses"].fillna(0)
            - standard_df["SG&A Expenses"].fillna(0)
        )
        # Add the column only if there are non-trivial values.
        if (other_op_ex.fillna(0).abs() > 1).any():
            standard_df["Other Operating Expenses"] = other_op_ex

    # --- Step 4: Assemble alternatives from the solver's cache for the UI ---
    alternatives = defaultdict(list)
    for std_name, possibilities in mapping.items():
        if std_name not in ["Gross Profit", "Operating Income", "Net Income"]:
            continue
        if not possibilities:
            continue
        metric_concept = possibilities[0]
        aggregator = defaultdict(dict)
        for year in df_indexed.index:
            results_for_year = all_results.get((metric_concept, year), [])
            for result in results_for_year:
                aggregator[result["source"]][year.isoformat()] = result["value"]

        formula_counter = 1
        for source, values_by_year in aggregator.items():
            if values_by_year:
                alternatives[std_name].append(
                    {"label": f"Formula {formula_counter}", "source": source, "values": values_by_year}
                )
                formula_counter += 1

    # --- Step 5: Final Formatting and Order ---
    final_order = [
        "Revenue", "Cost of Revenue", "Gross Profit", "R&D Expenses",
        "SG&A Expenses", "Other Operating Expenses",
        "Operating Income", "Interest Income",
        "Income from Equity Method Investments", "Other Non-operating Income (Expense)",
        "Income Before Taxes", "Taxes", "Net Income", "Basic EPS", "Diluted EPS",
    ]

    # Clean up the final dataframe
    final_df = standard_df[[col for col in final_order if col in standard_df.columns]]
    final_df = final_df.loc[:, (final_df.fillna(0) != 0).any(axis=0)]
    final_df.loc[(final_df.isnull()).all(axis=1)] = np.nan
    
    final_df = final_df.dropna(how="all", axis=0)

    # NEW: Create a list of the *raw* underlying metric names used in the final statement
    standard_metrics_raw = []
    for std_name in final_df.columns:
        # This logic mirrors the find_series helper to find the first available raw metric
        for p in mapping.get(std_name, []):
            col = find_col_with_units(p, df_indexed.columns)
            if col and col in df.columns:
                standard_metrics_raw.append(col)
                break # Found the primary metric for this standard name

    # Format the index to remove the timestamp, showing only the date.
    if not final_df.empty:
        final_df.index = pd.to_datetime(final_df.index).date

    return final_df, alternatives, list(final_df.columns)


def calculate_key_ratios(standard_is_df):
    """
    Calculates key profitability ratios from the standardized income statement.
    """
    if standard_is_df is None or standard_is_df.empty:
        return pd.DataFrame()

    ratios = pd.DataFrame(index=standard_is_df.index)

    # Use .get() to avoid KeyErrors if a column is missing
    revenue = standard_is_df.get("Revenue")
    gross_profit = standard_is_df.get("Gross Profit")
    operating_income = standard_is_df.get("Operating Income")
    net_income = standard_is_df.get("Net Income")

    # Gross Profit Margin
    if revenue is not None and gross_profit is not None:
        # Use .divide and handle potential division by zero
        ratios["Gross Profit Margin"] = gross_profit.divide(revenue).replace([np.inf, -np.inf], np.nan) * 100

    # Operating Profit Margin
    if revenue is not None and operating_income is not None:
        ratios["Operating Profit Margin"] = operating_income.divide(revenue).replace([np.inf, -np.inf], np.nan) * 100

    # Net Profit Margin
    if revenue is not None and net_income is not None:
        ratios["Net Profit Margin"] = net_income.divide(revenue).replace([np.inf, -np.inf], np.nan) * 100

    # Drop any rows where all ratios are NaN
    ratios.dropna(how='all', inplace=True)

    return ratios


def get_financial_reports(ticker):
    """
    Fetches all available 10-K filings for a given ticker and packages them into a
    single zip archive.

    This function performs the following steps:
    1. Retrieves the company's CIK from its ticker.
    2. Fetches the list of all company filings from the SEC.
    3. Filters this list to include only 10-K (annual) reports.
    4. For each 10-K filing, it constructs the URL to the `Financial_Report.xlsx`.
    5. It attempts to download each report. Since not all filings have an associated
       XLSX file (especially older ones), it validates the response.
    6. All successfully downloaded reports are written into a zip archive in memory.
    7. The zip archive is returned as a bytes object.

    Args:
        ticker (str): The stock ticker symbol for the company (e.g., 'AAPL').

    Returns:
        bytes or None: A bytes object representing the zip archive of 10-K reports,
                       or None if no reports could be downloaded.
    """
    # Retrieve the CIK for the given ticker.
    CIK = get_cik(ticker)
    if not CIK:
        raise ValueError(f"Could not find CIK for ticker {ticker}")

    # Set the User-Agent for SEC requests.
    headers = {"User-Agent": EMAIL}
    url = f"https://data.sec.gov/submissions/CIK{CIK}.json"

    # Fetch the filings metadata from the SEC.
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    data = response.json()

    # Filter for 10-K filings.
    df = pd.DataFrame(data["filings"]["recent"])
    df_10k = df[df["form"] == "10-K"]

    if df_10k.empty:
        return None  # No 10-K filings found.

    # Create an in-memory buffer for the zip file.
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_f:
        for _, entry in df_10k.iterrows():
            accession_number = entry["accessionNumber"].replace("-", "")
            filing_date = entry["filingDate"]
            report_url = f"https://www.sec.gov/Archives/edgar/data/{CIK}/{accession_number}/Financial_Report.xlsx"

            try:
                # Attempt to download the Excel report.
                report_response = requests.get(report_url, headers=headers)
                # Check for a successful response and that the content is an Excel file.
                if (
                    report_response.status_code == 200
                    and "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    in report_response.headers.get("Content-Type", "")
                ):
                    file_name = f"{ticker.upper()}_{entry['form']}_{filing_date}_Financial_Report.xlsx"
                    zip_f.writestr(file_name, report_response.content)
                else:
                    # Log if a report is not an xlsx file or if there's an error.
                    print(
                        f"Skipping non-xlsx or error response for {report_url} (status: {report_response.status_code})"
                    )
            except requests.exceptions.RequestException as e:
                # Log any network-related errors during download.
                print(f"Could not download report from {report_url}: {e}")
                continue

    # Only return the buffer if files were actually added to the zip.
    if zip_f.namelist():
        zip_buffer.seek(0)
        return zip_buffer.getvalue()
    else:
        return None  # Return None if no files were successfully downloaded.
