import pandas as pd
import numpy as np
import requests
import os
import json
from collections import defaultdict
from functools import reduce

# data_loader.py
# This script is responsible for fetching, processing, and enriching financial data from the SEC EDGAR database.
# It contains the core logic for parsing XBRL data, applying calculation formulas to fill missing values,
# and creating standardized financial statements.

EMAIL = "your_email@example.com"

# Load the CIK dictionary, which maps company tickers to their Central Index Key.
CIK_dict = pd.read_csv("CIK_dict.csv", converters={"cik_str": str})

# Load the comprehensive set of calculation formulas generated by xbrl_main.py.
# This file contains the parent-child relationships for financial metrics as defined in the US-GAAP XBRL taxonomy.
with open("xbrl_prep/income_statement_formulas.json", "r") as f:
    calculation_formulas = json.load(f)

# The inverted index is needed again to find all possible calculation paths for the UI.
calculation_formulas_inverted = defaultdict(list)
for parent, formulas in calculation_formulas.items():
    for formula in formulas:
        for child_info in formula["children"]:
            child_name = child_info["child"]
            calculation_formulas_inverted[child_name].append(parent)


def get_company_info(ticker):
    """
    Retrieves company info (title, CIK) for a given ticker symbol.
    """
    row = CIK_dict[CIK_dict["ticker"].str.upper() == ticker.upper()]
    if not row.empty:
        return {
            "title": row["title"].iloc[0],
            "cik_str": row["cik_str"].iloc[0],
        }
    return None


def get_cik(ticker):
    """
    Retrieves the Central Index Key (CIK) for a given company ticker symbol.
    """
    info = get_company_info(ticker)
    if info:
        return info["cik_str"]
    else:
        raise ValueError(f"Ticker {ticker} not found in CIK_dict")


def get_filing_by_metrics(CIK):
    """
    Fetches a company's entire fact history from the SEC EDGAR API.

    Args:
        CIK (str): The company's Central Index Key.

    Returns:
        dict: A dictionary containing all the us-gaap facts for the company.
    """
    url = f"https://data.sec.gov/api/xbrl/companyfacts/CIK{CIK}.json"
    headers = {"User-Agent": EMAIL}
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raise an exception for HTTP errors
    data = response.json()
    return data["facts"]["us-gaap"]


def extract_metrics(filing_metrics):
    """
    Extracts and filters all 10-K (annual report) data points from the raw SEC filing.

    Args:
        filing_metrics (dict): The raw dictionary of facts from the SEC API.

    Returns:
        dict: A cleaned dictionary where keys are metric names (with units, e.g., "Revenues_USD")
              and values are lists of data points, exclusively from 10-K filings.
    """
    metric_master = {}
    for metric, attributes in filing_metrics.items():
        for unit, entries in attributes["units"].items():
            for entry in entries:
                # We are only interested in annual data, which is reported in 10-K forms.
                if entry["form"] == "10-K":
                    metric_key = f"{metric}_{unit}"
                    if metric_key not in metric_master:
                        metric_master[metric_key] = []
                    metric_master[metric_key].append(entry)
    return metric_master


def format_metrics_efficient(extracted_metrics):
    """
    Converts extracted metrics into a clean, graphable DataFrame.
    This version is both efficient and accurate, correctly handling duplicate years
    by selecting the most recent filing.
    """
    # Filter out entries with 'frame' attribute to avoid aggregated/inconsistent data
    filtered_metrics = {}

    for metric_name, metric_entries in extracted_metrics.items():
        # Filter out entries that have a 'frame' attribute
        filtered_entries = [
            entry
            for entry in metric_entries
            if "frame" not in entry
            or (
                "frame" in entry
                and "Q" not in entry["frame"]
                and "q" not in entry["frame"]
            )
        ]
        if filtered_entries:  # Only include metrics that have valid entries
            filtered_metrics[metric_name] = filtered_entries

    # Process each metric
    all_entries = []
    for metric_name, entries in extracted_metrics.items():
        for entry in entries:
            if "start" in entry:
                # If 'start' exists, create a new dictionary for the entry with standardized keys.
                # This flattens the data for easier processing or conversion into a DataFrame.
                all_entries.append(
                    {
                        "fy": entry["fy"],  # Fiscal year of the entry.
                        "metric": metric_name,  # The name of the financial metric.
                        "val": entry["val"],  # The value of the metric.
                        "filed": entry[
                            "filed"
                        ],  # The date the metric was filed, useful for finding the latest entry.
                        "end": entry["end"],  # The end date/period of the entry.
                        "start": entry["start"],  # The start date/period of the entry.
                    }
                )
            else:
                all_entries.append(
                    {
                        "fy": entry["fy"],  # Fiscal year of the entry.
                        "metric": metric_name,  # The name of the financial metric.
                        "val": entry["val"],  # The value of the metric.
                        "filed": entry[
                            "filed"
                        ],  # The date the metric was filed, useful for finding the latest entry.
                        "end": entry["end"],  # The end date/period of the entry.
                    }
                )

    if not all_entries:
        return pd.DataFrame()

    # Create a single DataFrame from all data points at once
    df = pd.DataFrame(all_entries)

    # Convert 'filed' to datetime for correct sorting
    df["filed"] = pd.to_datetime(df["filed"])
    df["end"] = pd.to_datetime(df["end"])

    if "start" in df.columns:
        df["start"] = pd.to_datetime(df["start"])

    # Sort by filing date to ensure the latest filing comes first for each metric/year
    df.sort_values(
        by=["fy", "filed", "end", "start"],
        ascending=[True, False, True, False],
        inplace=True,
    )

    # Drop duplicates, keeping the last entry which is the most recent filing
    df.drop_duplicates(subset=["metric", "fy"], keep="last", inplace=True)

    # Pivot the DataFrame to get fiscal years as the index and metrics as columns
    df_pivot = df.pivot(index="fy", columns="metric", values="val")

    # Apply the deep recursive solver to intelligently fill in missing values.
    # This is the core of the data enrichment process.
    df_solved, all_results = apply_formulas_deep_dfs(
        df_pivot,
        calculation_formulas,
    )

    # If the solving process creates duplicate columns (e.g., from different XBRL roles),
    # group by the column names and take the first non-null value for each year.
    df_solved = df_solved.groupby(by=df_solved.columns, axis=1).first()

    # Reset index to make 'fy' a regular column for easier plotting.
    df_solved.reset_index(inplace=True)
    df_solved.rename_axis(None, axis=1, inplace=True)

    return df_solved, all_results


def find_col_with_units(base_metric, df_columns):
    """
    Finds the full column name in a DataFrame that corresponds to a base metric name.
    It prioritizes '_USD' suffixes as they are the most common for financial statements.

    Args:
        base_metric (str): The base name of the metric (e.g., "Revenues").
        df_columns (list): The list of all column names in the DataFrame.

    Returns:
        str or None: The full column name if found, otherwise None.
    """
    # Prioritize metrics with _USD unit as they are most common for income statements
    if base_metric + "_USD" in df_columns:
        return base_metric + "_USD"
    for col in df_columns:
        if col.startswith(base_metric + "_"):
            return col
    if base_metric in df_columns:
        return base_metric
    return None


def apply_formulas_deep_dfs(df, all_formulas):
    """
    Applies XBRL calculation formulas using a recursive Depth-First Search (DFS) algorithm.
    It now calculates all viable formulas for each metric and stores them for the UI.
    """
    df_enriched = df.copy()
    cache = {}
    # This new cache will store all successful calculation results for each metric/year.
    all_results_cache = defaultdict(list)

    def get_sorted_formulas(formulas, year, visited):
        """
        Scores a list of candidate formulas and returns them sorted from best to worst.
        The score is a tuple that prioritizes formulas with more children already present
        in the raw data, then by the number of solvable children, and finally by shallowness.
        """
        scored_formulas = []
        for formula_obj in formulas:
            child_list = formula_obj["formula"]
            total_children = len(child_list)
            if total_children == 0:
                continue

            present_in_df = 0
            solvable = 0

            # If any required component is already in the current recursion stack, this
            # formula would cause a cycle, so we disqualify it immediately.
            is_cyclic = any((child["child"], year) in visited for child in child_list)
            if is_cyclic:
                continue

            for child in child_list:
                child_name = child["child"]
                col_name = find_col_with_units(child_name, df_enriched.columns)

                # Highest priority: is the data already in the dataframe?
                if col_name and pd.notnull(df_enriched.at[year, col_name]):
                    present_in_df += 1
                # Second priority: is it solvable via another formula?
                elif child_name in all_formulas:
                    solvable += 1

            # The score is a tuple that prioritizes existing data above all.
            # Python's max() on a list of tuples will sort by each element in order.
            score = (present_in_df, solvable, -total_children)
            scored_formulas.append({"formula_obj": formula_obj, "score": score})

        # Sort the formulas from best to worst score.
        return sorted(scored_formulas, key=lambda x: x["score"], reverse=True)

    def solve_for_cell(metric_base, year, visited):
        """
        Recursively attempts to solve for a single metric in a single year (a "cell").
        It calculates all viable formulas, stores them, and returns the highest-scored result.
        """
        # --- Base Cases and Memoization ---
        if (metric_base, year) in visited:
            return None  # Cycle detected
        visited.add((metric_base, year))

        if (metric_base, year) in cache:
            return cache.get((metric_base, year))

        col_name = find_col_with_units(metric_base, df_enriched.columns)
        if col_name and pd.notnull(df_enriched.at[year, col_name]):
            val = df_enriched.at[year, col_name]
            cache[(metric_base, year)] = val
            return val

        # --- Forward Calculation (Solving a parent from its children) ---
        primary_result = None
        if metric_base in all_formulas:
            candidate_formulas = [
                {"type": "forward", "formula": f["children"], "parent": metric_base}
                for f in all_formulas[metric_base]
            ]
            sorted_formulas = get_sorted_formulas(candidate_formulas, year, visited)

            # Iterate through sorted formulas, calculate all viable results, and store them.
            for scored_formula in sorted_formulas:
                formula_obj = scored_formula["formula_obj"]
                child_values = []
                all_children_solved = True

                formula_parts = []
                for child in formula_obj["formula"]:
                    val = solve_for_cell(child["child"], year, visited.copy())
                    if val is None:
                        all_children_solved = False
                        break
                    child_values.append({"value": val, "weight": child["weight"]})

                    # Build formula string for display
                    sign = "+" if child["weight"] > 0 else "-"
                    child_name_part = child["child"]
                    weight_part = (
                        f"{abs(child['weight'])} * "
                        if abs(child["weight"]) != 1
                        else ""
                    )
                    formula_parts.append(f"{sign} {weight_part}{child_name_part}")

                if all_children_solved:
                    result = sum(cv["value"] * cv["weight"] for cv in child_values)
                    formula_str = " ".join(formula_parts).lstrip("+ ")

                    # Store this successful result in the new cache.
                    all_results_cache[(metric_base, year)].append(
                        {"source": f"Calc: {formula_str}", "value": result}
                    )

                    # The first successful result (from the best-scored formula) is the primary one.
                    if primary_result is None:
                        primary_result = result

        # Cache and return only the primary result for other solver calculations.
        cache[(metric_base, year)] = primary_result

        if primary_result is not None:
            return primary_result

        # If we get here, no formula succeeded. Assume 0 if it's a leaf node.
        if metric_base not in all_formulas:
            cache[(metric_base, year)] = 0.0
            return 0.0

        # If it's not a leaf node but all formulas failed, return None.
        return None

    # --- Main Execution Loop ---
    # THE CRITICAL FIX: The solver must consider ALL metrics available in the data AND
    # the formulas as potential starting points. This ensures that even if a metric
    # in the data isn't a parent in the formulas, its value is still available
    # for calculations where it acts as a child.
    all_data_metrics = {
        col.replace("_USD", "").split("_")[0] for col in df_enriched.columns
    }
    all_formula_metrics = set(list(all_formulas.keys()))
    all_metrics_to_solve = all_data_metrics.union(all_formula_metrics)

    for metric in all_metrics_to_solve:
        col_name = find_col_with_units(metric, df_enriched.columns)
        # Optimization: If the column already exists and is fully populated, skip it.
        if col_name and df_enriched[col_name].notna().all():
            continue

        for year in df_enriched.index:
            # Only try to solve if the value is actually missing.
            if not (col_name and pd.notnull(df_enriched.at[year, col_name])):
                # The `top_level_target` is no longer needed.
                solved_value = solve_for_cell(metric, year, set())
                if solved_value is not None:
                    # IMPORTANT: Update the DataFrame immediately after solving a cell.
                    # This makes the newly calculated value available for subsequent calculations
                    # in the same run, which is critical for complex dependencies.
                    final_col_name = find_col_with_units(
                        metric, df_enriched.columns
                    ) or (metric + "_USD")
                    if final_col_name not in df_enriched:
                        df_enriched[final_col_name] = np.nan
                    df_enriched.at[year, final_col_name] = solved_value

    return df_enriched, all_results_cache


def process_metrics(ticker: str):
    """
    A convenience wrapper function that chains together all the steps required
    to fetch and process financial data for a given ticker.
    This now returns the comprehensive solved dataframe, the standard income statement,
    and a dictionary of alternative calculations.
    """
    df_raw_extracted = extract_metrics(get_filing_by_metrics(get_cik(ticker)))

    # format_metrics_efficient now returns the solved df with all metrics
    df_solved, all_results = format_metrics_efficient(df_raw_extracted)

    # create_standard_income_statement takes the solved df and the cache
    standard_is_df, alternatives, standard_metrics_list = create_standard_income_statement(
        df_solved, all_results
    )

    return df_solved, standard_is_df, alternatives, standard_metrics_list


def create_standard_income_statement(df, all_results):
    """
    Creates a standardized income statement from the fully solved financial data.
    This function now also calculates and returns a dictionary of all possible alternative
    calculation results for each line item, which can be surfaced in the UI.
    """
    if df is None or df.empty:
        return pd.DataFrame(), {}

    df_indexed = df.copy()
    if "fy" in df_indexed.columns:
        df_indexed = df_indexed.set_index("fy").sort_index()

    # This mapping defines the structure of our standardized statement.
    mapping = {
        "Revenue": ["Revenues"],
        "Cost of Revenue": ["CostOfRevenue"],
        "Gross Profit": ["GrossProfit"],
        "R&D Expenses": ["ResearchAndDevelopmentExpense"],
        "SG&A Expenses": ["SellingGeneralAndAdministrativeExpense"],
        "Operating Income": ["OperatingIncomeLoss"],
        "Interest Income": ["InterestIncomeExpenseNet"],
        "Interest Expense": ["InterestExpense"],
        "Income Before Taxes": [
            "IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest"
        ],
        "Taxes": ["IncomeTaxExpenseBenefit"],
        "Net Income": ["NetIncomeLoss"],
    }

    # --- Step 1: Build the default standardized DataFrame ---
    standard_df = pd.DataFrame(index=df_indexed.index)
    for std_name, possibilities in mapping.items():
        # Find the first available column from the possibilities with a _USD suffix.
        found_col = find_col_with_units(possibilities[0], df_indexed.columns)
        if found_col and found_col in df_indexed.columns:
            standard_df[std_name] = df_indexed[found_col]
        else:
            standard_df[std_name] = 0.0

    # --- Step 2: Assemble alternatives from the solver's cache ---
    alternatives = defaultdict(list)
    for std_name, possibilities in mapping.items():
        metric_concept = possibilities[0]

        # Temp dict to group results by their source formula string
        # e.g., {'Calc: A + B': {2020: 100, 2021: 110}, ...}
        aggregator = defaultdict(dict)

        for year in df_indexed.index:
            results_for_year = all_results.get((metric_concept, year), [])
            for result in results_for_year:
                aggregator[result["source"]][year] = result["value"]

        # Format the aggregated results for the UI, creating a simple numbered label for each.
        formula_counter = 1
        for source, values_by_year in aggregator.items():
            alternatives[std_name].append(
                {
                    "label": f"Formula {formula_counter}",  # Simple label for the UI
                    "source": source,  # Full formula string is the unique ID
                    "values": values_by_year,
                }
            )
            formula_counter += 1

    # --- Step 3: Final Formatting and Plug Calculation for the default view ---
    final_order = [
        "Revenue",
        "Cost of Revenue",
        "Gross Profit",
        "R&D Expenses",
        "SG&A Expenses",
        "Operating Income",
        "Interest Income",
        "Interest Expense",
        "Income Before Taxes",
        "Taxes",
        "Net Income",
    ]

    # Calculate 'Other Operating Expenses' as a final plug to make the statement balance
    if all(
        c in standard_df.columns
        for c in ["Gross Profit", "Operating Income", "R&D Expenses", "SG&A Expenses"]
    ):
        standard_df["Other Operating Expenses"] = (
            standard_df["Gross Profit"]
            - standard_df["Operating Income"]
            - standard_df["R&D Expenses"]
            - standard_df["SG&A Expenses"]
        )
        final_order.insert(5, "Other Operating Expenses")

    final_df = standard_df[[col for col in final_order if col in standard_df.columns]]

    # Replace any rows that are all zeros with NaN so they can be dropped
    final_df.loc[(final_df.T == 0).all()] = np.nan

    return final_df.dropna(how="all", axis=0), alternatives, final_order
